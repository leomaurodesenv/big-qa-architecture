# -*- coding: utf-8 -*-
"""Finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PRCbaNy2B4vs_3oO2PaOSswJPZUL0PTD
"""

# pip install datasets
# pip install transformers
# pip install huggingface_hub
# pip install evaluate
# pip install torch

import os
import enum
import ast
from utils import HUGGINGFACE_TOKEN, DocReader, Sports

from datasets import load_dataset, Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForQuestionAnswering,
    DefaultDataCollator,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    pipeline,
)
from huggingface_hub import HfApi, login

DOC_READER = DocReader.DistilBERT
SPORT = Sports.BASKETBALL


os.environ["HUGGINGFACE_TOKEN"] = HUGGINGFACE_TOKEN
login(token=HUGGINGFACE_TOKEN)


# Carregar dataset
dataset_train = load_dataset("PedroCJardim/QASports", SPORT, split="train")
dataset_validation = load_dataset("PedroCJardim/QASports", SPORT, split="validation")

# Carregar tokenizador
tokenizer = AutoTokenizer.from_pretrained(DOC_READER)
model = AutoModelForQuestionAnswering.from_pretrained(DOC_READER)


# Pipeline completo de pre process
cleaned_data_train = load_and_clean_data(dataset_train)
cleaned_data_validation = load_and_clean_data(dataset_validation)
processed_data_train = cleaned_data.map(preprocess_function, batched=True)
processed_data_validation = cleaned_data_validation.map(
    preprocess_function, batched=True
)
filtered_data_train = filter_dataset(processed_data)
filtered_data_validation = filter_dataset(processed_data2)

# verificação do tamanho dos vetores pre processados
print(
    f"Original: {len(dataset_train)}, Limpo: {len(cleaned_data_train)}, Filtrado: {len(filtered_data_train)}"
)
print(
    f"Original: {len(dataset_validation)}, Limpo: {len(cleaned_data_validation)}, Filtrado: {len(filtered_data_validation)}"
)

# verificação de um vetor
print(filtered_data_validation[0])

data_collator = DefaultDataCollator()

training_args = TrainingArguments(
    output_dir="distilbert-qasports",
    evaluation_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    learning_rate=1e-5,
    per_device_train_batch_size=16,
    gradient_accumulation_steps=2,
    per_device_eval_batch_size=16,
    num_train_epochs=50,
    weight_decay=0.01,
    push_to_hub=True,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    logging_steps=100,
    fp16=True,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=filtered_data_train,  # Dataset de treino processado
    eval_dataset=filtered_data_validation,  # Dataset de validação processado
    data_collator=data_collator,  # Collator correto
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],
)

trainer.train()

# Salvar tokenizer localmente
# tokenizer.save_pretrained("distilbert-qasports")

# Enviar tokenizer para o Hugging Face
from huggingface_hub import HfApi

api = HfApi()
api.upload_folder(
    folder_path="distilbert-qasports",
    repo_id="laurafcamargos/distilbert-qasports",
    commit_message="Adicionando tokenizer",
)

# Publicar modelo no HuggingFace
# trainer.push_to_hub()

# Inferência
question = "Who owns the building?"
context = "(See Memphis Summer Storm of 2003.) It was built at a cost of US$250 million and is owned by the City of Memphis, naming rights were purchased by one of Memphis' most well-known businesses, FedEx, for $92 million. FedExForum was financed using $250 million of public bonds, which were issued by the Memphis Public Building Authority (PBA)."

question_answerer = pipeline(
    task="question-answering", model=model, tokenizer=tokenizer
)
answer = question_answerer(question=question, context=context)

print(answer)
